import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

'''
train dataset has 42000 rows, and 785 columns. 
Rows are examples
1st column is the label (answers)
All the rest(784) columns are pixel values

test set has 28000 rows, and 784 columns
test set does not include the answer column.
'''

train = train.values.T # turn train set dataframe to numpy array form so it is easier to work with and scale it.
test = test.values.T
y_values = train[[0],:] # bring y values  [3,1,4,6,2,0, ... ]
train = train[1:,:]

y = np.zeros((10,y_values.shape[1])) 
for i in range(y_values.shape[1]):
    y[y_values[0][i]][i] = 1

print('y : ',y.shape)
print('train : ',train.shape)
print('test : ',test.shape)

# scaling dataset values to range (0,1)
    
train = train/255
test = test/255

# %% [code]
def sigmoid(matrix):
    s = 1 / (1 + np.exp(-matrix))
    return s

def relu(matrix):
    matrix = matrix * (matrix > 0)
    return matrix

def zero_init(dim):
    zero_vector = np.zeros((dim, 1))
    return zero_vector

def he_init(layer_dim): #type : list
    weights = {}
    for i in range(1,len(layer_dim)):
        weights['W'+str(i)] = np.random.randn(layer_dim[i],layer_dim[i-1]) * np.sqrt(1/layer_dim[i-1])
        weights['b'+str(i)] = zero_init(layer_dim[i])
    return weights

def forward_propagation(w,b,a,activation): # w : weights b : bias a : activation from previous layer
    z = np.dot(w, a) + b
    
    if activation == 'sigmoid':
        a = sigmoid(z)
    elif activation == 'relu':
        a = relu(z)
    return z,a

def cost_function(a,y): # a : activation from last layer, y : labels 
    cost = np.zeros((a.shape[0],1))
    for i in range(a.shape[0]):
        entry = -(np.dot(y[i][:],np.log(a[i][:]).T) + np.dot((1-y[i][:]),np.log(1-a[i][:]).T))/y.shape[1]
        cost[i][0] = entry
    cost = np.linalg.norm(cost, ord=2)
    return cost
    
def forward_propagation_full(weights, layer_dim, data, y):
    cache = {}
    for i in range(1,len(layer_dim)):
        w = weights['W'+str(i)]
        b = weights['b'+str(i)]
        if i == 1:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,data,'relu')
        elif i == len(layer_dim)-1:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,cache['a'+str(i-1)],'sigmoid')
        else:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,cache['a'+str(i-1)],'relu')
    
    cost = cost_function(cache['a'+str((len(layer_dim)-1))], y)
    return cost, cache

def backward_propagation(da, a_prev, z, w, b, activation_method):
    if activation_method == 'sigmoid':
        s = sigmoid(z)
        dz = da * s * (1-s)
    
    elif activation_method == 'relu':
        s = relu(z)
        s[s<0] = 0
        dz = s
        
    m = a_prex.shape[1]
    dw = np.dot(dz, a_prev.T)
    da_prev = np.dot(w.T, dz)
    db = np.sum(dz, axis = 0)/m
    
    return da, dw, db

def backward_propagation_full(cost, cache, weights, train, y):
    daL = -y/aL + (1-y)/(1-aL)
    
    for iteration in reversed(range(1, len(layer_dims))):
        
        
        da_prev, dw, db = backward_propagation()
        grads['da'+str(iteration)] = da_prev
        grads['dw'+str(iteration)] = dw
        grads['db'+str(iteration)] = db
    return

def update_parameter(w, b, train, y, num_iteration, learning_rate = 0.01, show_cost = False):
    print('update parameter')
    for i in range(num_iteration):
        print('i : ',i)
        print('backward')
        dw, db = backward_propagation(w, b, train, y)
        print('forward')
        cost = forward_propagation(w,b,train)['cost']
        print('update')
        w = w - learning_rate*dw
        b = b - learning_rate*db.to_numpy()
        if i % 100 and show_cost == True:
            print('cost after {} iteration :'.format(i), cost)
    return w,b

def predict(dataset,w,b):
    a = forward_propagation(w,b,train)['a']
    for i in range(a.shape[1]):
        if a[0][i] > 0.5:
            prediction[0][i] = 1
        else:
            prediction[0][i] = 0
    return prediction

def model(train, y, show_loss = True, num_iter = 2000):
    w = zeros(train.shape[0])
    b = np.array([0])
    print('here we go')
    w, b = update_parameter(w, b, train, y, num_iter)
    print('predictions')
    prediction = predict(train, w,b)
    print('show cost')
    if show_loss == True:
        print(loss(w, b, train, y))
