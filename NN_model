import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib as plt

'''
train dataset has 42000 rows, and 785 columns. 
Rows are examples
1st column is the label (answers)
All the rest(784) columns are pixel values

test set has 28000 rows, and 784 columns
test set does not include the answer column.
'''

train = train.values.T # turn train set dataframe to numpy array form so it is easier to work with and scale it.
test = test.values.T
y_values = train[[0],:] # bring y values  [3,1,4,6,2,0, ... ]
train = train[1:,:]

y = np.zeros((10,y_values.shape[1])) 
for i in range(y_values.shape[1]):
    y[y_values[0][i]][i] = 1 # one-hot encoding

print('y : ',y.shape)
print('train : ',train.shape)
print('test : ',test.shape)

# scaling dataset values to range (0,1)
train = train/255
test = test/255


def sigmoid(matrix):
    s = 1 / (1 + np.exp(-matrix))
    return s

def relu(matrix):
    matrix = matrix * (matrix > 0)
    return matrix

def zero_init(dim):
    zero_vector = np.zeros((dim, 1))
    return zero_vector

def he_init(layer_dim): #type : list
    parameters = {}
    L = len(layer_dim)
    for i in range(1,L):
        parameters['w'+str(i)] = np.random.randn(layer_dim[i],layer_dim[i-1]) * np.sqrt(1/layer_dim[i-1])
        parameters['b'+str(i)] = zero_init(layer_dim[i])
    return parameters

def forward_propagation(w,b,a,activation): # w : weights b : bias a : activation from previous layer
    z = np.dot(w, a) + b
    
    if activation == 'sigmoid':
        a = sigmoid(z)
    elif activation == 'relu':
        a = relu(z)
    return z,a
    
def forward_propagation_full(parameters, layer_dim, x, y):
    cache = {}
    cache['a'+str(0)] = x
    L = len(layer_dim)
    for i in range(1,L):
        w = parameters['w'+str(i)]
        b = parameters['b'+str(i)]
        if i == L-1:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,cache['a'+str(i-1)],'sigmoid')            
        else:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,cache['a'+str(i-1)],'relu')
    
    aL = cache['a'+str(L-1)]
    return aL, cache

def cost_function(aL,y): # a : activation from final layer, y : labels 
    cost = np.zeros((aL.shape[0],1))
    for i in range(y.shape[0]):
        cost[i][0] = -(np.dot(y[i][:],np.log(aL[i][:]).T) + np.dot((1-y[i][:]),np.log(1-aL[i][:]).T))/y.shape[1]
    cost = np.linalg.norm(cost, ord=2)
    return cost

def backward_propagation(da, a_prev, z, w, b, activation_method):
    if activation_method == 'sigmoid':
        s = sigmoid(z)
        dz = da * s * (1-s)
    
    elif activation_method == 'relu':
        s = relu(z)
        s[s<0] = 0
        dz = s
        
    m = a_prev.shape[1]
    dw = np.dot(dz, a_prev.T)
    da_prev = np.dot(w.T, dz)
    db = np.sum(dz, axis = 1, keepdims = True)/m
    return da_prev, dw, db

def backward_propagation_full(x, y, parameters, cache):
    grads = {}
    L = int(len(parameters)/2)
    aL = cache['a'+str(L)]
    daL = -y/aL + (1-y)/(1-aL)
    grads['da'+str(L)] = daL
    
    flag = False
    for i in reversed(range(1,L+1)): #3,2,1
        if flag == False:
            activation_method = 'sigmoid'
            flag = True
        else: 
            activation_method = 'relu'
        
        da_prev, dw, db = backward_propagation(grads['da'+str(i)], cache['a'+str(i-1)], cache['z'+str(i)], 
                                               parameters['w'+str(i)], parameters['b'+str(i)], activation_method)
        grads['dw'+str(i)] = dw
        grads['db'+str(i)] = db
        grads['da'+str(i-1)] = da_prev
                            
        #print('backprop checking')
        #print('dw'+str(i), grads['dw'+str(i)].shape)
        #print('db'+str(i), grads['db'+str(i)].shape)
        #print('da'+str(i), grads['da'+str(i)].shape)
        
        
    return grads

def update_parameter(parameters, grads, optimizer, learning_rate):
    L = int(len(parameters)/2)
    if optimizer == 'fullBatch':
        for i in range(L):
            parameters['w'+str(i+1)] -= learning_rate*grads['dw'+str(i+1)]
            parameters['b'+str(i+1)] -= learning_rate*grads['db'+str(i+1)]
    elif optimizer == 'momentum':
        pass
    elif optimizer == 'adam':
        pass
    return parameters



def model(x, y, layer_dims, num_iter = 500, optimizer = 'fullBatch', show_cost = True, learning_rate = 1):
    parameters = he_init(layer_dims)
    
    for iteration in range(1,num_iter+1):
        aL, cache = forward_propagation_full(parameters, layer_dim, x, y)        
        cost = cost_function(aL,y)
        grads = backward_propagation_full(x, y, parameters, cache)
        parameters = update_parameter(parameters, grads, optimizer,learning_rate)
        
        if iteration % 10 and show_cost == True:
            print('cost after {} iteration :'.format(iteration), cost)
    
    #plt.plot(costs)
    #plt.ylabel('cost')
    #plt.xlabel('epochs (per 100)')
    #plt.title("Learning rate = " + str(learning_rate))
    #plt.show()

    return parameters

def predict(parameters, layer_dim, train, y):
    activation, cache = forward_propagation_full(parameters, layer_dim, train, y)
    prediction = activation.argmax(0)
    data_to_submit = pd.DataFrame(prediction.T, columns=['prediction'])
    return data_to_submit
