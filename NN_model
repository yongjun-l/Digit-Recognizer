def sigmoid(matrix):
    s = 1 / (1 + np.exp(-matrix))
    return s

def relu(matrix):
    matrix = matrix * (matrix > 0)
    return matrix

def zero_init(dim):
    zero_vector = np.zeros((dim, 1))
    return zero_vector

def he_init(layer_dim): #type : list
    parameters = {}
    L = len(layer_dim)
    for i in range(1,L):
        parameters['w'+str(i)] = np.random.randn(layer_dim[i],layer_dim[i-1]) * np.sqrt(1/layer_dim[i-1])
        parameters['b'+str(i)] = zero_init(layer_dim[i])
    return parameters

def forward_propagation(w,b,a,activation): # w : weights b : bias a : activation from previous layer
    z = np.dot(w, a) + b
    
    if activation == 'sigmoid':
        a = sigmoid(z)
    elif activation == 'relu':
        a = relu(z)
    return z,a
    
def forward_propagation_full(parameters, layer_dim, x, y):
    cache = {}
    cache['a'+str(0)] = x
    L = len(layer_dim)
    for i in range(1,L):
        w = parameters['w'+str(i)]
        b = parameters['b'+str(i)]
        if i == L-1:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,cache['a'+str(i-1)],'sigmoid')            
        else:
            cache['z'+str(i)], cache['a'+str(i)] = forward_propagation(w,b,cache['a'+str(i-1)],'relu')
    
    aL = cache['a'+str(L-1)]
    return aL, cache

def cost_function(aL,y): # a : activation from final layer, y : labels 
    cost = np.zeros((aL.shape[0],1))
    for i in range(y.shape[0]):
        cost[i][0] = -(np.dot(y[i][:],np.log(aL[i][:]).T) + np.dot((1-y[i][:]),np.log(1-aL[i][:]).T))/y.shape[1]
    cost = np.linalg.norm(cost, ord=2)
    return cost

def backward_propagation(da, a_prev, z, w, b, activation_method):
    if activation_method == 'sigmoid':
        s = sigmoid(z)
        dz = da * s * (1-s)
    
    elif activation_method == 'relu':
        s = relu(z)
        s[s<0] = 0
        dz = s
        
    m = a_prev.shape[1]
    dw = np.dot(dz, a_prev.T)
    da_prev = np.dot(w.T, dz)
    db = np.sum(dz, axis = 1, keepdims = True)/m
    return da_prev, dw, db

def backward_propagation_full(x, y, parameters, cache):
    grads = {}
    L = int(len(parameters)/2)
    aL = cache['a'+str(L)]
    daL = -y/aL + (1-y)/(1-aL)
    grads['da'+str(L)] = daL
    
    flag = False
    for i in reversed(range(1,L+1)): #3,2,1
        if flag == False:
            activation_method = 'sigmoid'
            flag = True
        else: 
            activation_method = 'relu'
        
        da_prev, dw, db = backward_propagation(grads['da'+str(i)], cache['a'+str(i-1)], cache['z'+str(i)], 
                                               parameters['w'+str(i)], parameters['b'+str(i)], activation_method)
        grads['dw'+str(i)] = dw
        grads['db'+str(i)] = db
        grads['da'+str(i-1)] = da_prev
                            
        #print('backprop checking')
        #print('dw'+str(i), grads['dw'+str(i)].shape)
        #print('db'+str(i), grads['db'+str(i)].shape)
        #print('da'+str(i), grads['da'+str(i)].shape)
        
        
    return grads

def update_parameter(parameters, grads, optimizer, learning_rate):
    L = int(len(parameters)/2)
    if optimizer == 'fullBatch':
        for i in range(L):
            parameters['w'+str(i+1)] -= learning_rate*grads['dw'+str(i+1)]
            parameters['b'+str(i+1)] -= learning_rate*grads['db'+str(i+1)]
    elif optimizer == 'momentum':
        pass
    elif optimizer == 'adam':
        pass
    return parameters



def model(x, y, layer_dims, num_iter = 500, optimizer = 'fullBatch', show_cost = True, learning_rate = 1):
    parameters = he_init(layer_dims)
    
    for iteration in range(1,num_iter+1):
        aL, cache = forward_propagation_full(parameters, layer_dim, x, y)        
        cost = cost_function(aL,y)
        grads = backward_propagation_full(x, y, parameters, cache)
        parameters = update_parameter(parameters, grads, optimizer,learning_rate)
        
        if iteration % 10 and show_cost == True:
            print('cost after {} iteration :'.format(iteration), cost)
    
    #plt.plot(costs)
    #plt.ylabel('cost')
    #plt.xlabel('epochs (per 100)')
    #plt.title("Learning rate = " + str(learning_rate))
    #plt.show()

    return parameters

def predict(parameters, layer_dim, train, y):
    activation, cache = forward_propagation_full(parameters, layer_dim, train, y)
    prediction = activation.argmax(0)
    data_to_submit = pd.DataFrame(prediction.T, columns=['prediction'])
    return data_to_submit
